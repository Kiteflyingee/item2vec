Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:

class WWoorrdd22VVeeccKKeeyyeeddVVeeccttoorrss(WordEmbeddingsKeyedVectors)
 |  Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.
 |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.
 |  
 |  Method resolution order:
 |      Word2VecKeyedVectors
 |      WordEmbeddingsKeyedVectors
 |      BaseKeyedVectors
 |      gensim.utils.SaveLoad
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  ggeett__kkeerraass__eemmbbeeddddiinngg(self, train_embeddings=False)
 |      Get a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings.
 |      
 |      Parameters
 |      ----------
 |      train_embeddings : bool
 |          If False, the weights are frozen and stopped from being updated.
 |          If True, the weights can/will be further trained/updated.
 |      
 |      Returns
 |      -------
 |      `keras.layers.Embedding`
 |          Embedding layer.
 |      
 |      Raises
 |      ------
 |      ImportError
 |          If `Keras <https://pypi.org/project/Keras/>`_ not installed.
 |      
 |      Warnings
 |      --------
 |      Current method work only if `Keras <https://pypi.org/project/Keras/>`_ installed.
 |  
 |  ssaavvee__wwoorrdd22vveecc__ffoorrmmaatt(self, fname, fvocab=None, binary=False, total_vec=None)
 |      Store the input-hidden weight matrix in the same format used by the original
 |      C word2vec-tool, for compatibility.
 |      
 |      Parameters
 |      ----------
 |      fname : str
 |          The file path used to save the vectors in
 |      fvocab : str, optional
 |          Optional file path used to save the vocabulary
 |      binary : bool, optional
 |          If True, the data will be saved in binary word2vec format, else it will be saved in plain text.
 |      total_vec : int, optional
 |          Optional parameter to explicitly specify total no. of vectors
 |          (in case word vectors are appended with document vectors afterwards).
 |  
 |  ----------------------------------------------------------------------
 |  Class methods defined here:
 |  
 |  llooaadd(fname_or_handle, **kwargs) from builtins.type
 |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.
 |      
 |      Parameters
 |      ----------
 |      fname : str
 |          Path to file that contains needed object.
 |      mmap : str, optional
 |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays
 |          via mmap (shared memory) using `mmap='r'.
 |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.
 |      
 |      See Also
 |      --------
 |      :meth:`~gensim.utils.SaveLoad.save`
 |          Save object to file.
 |      
 |      Returns
 |      -------
 |      object
 |          Object loaded from `fname`.
 |      
 |      Raises
 |      ------
 |      AttributeError
 |          When called on an object instance instead of class (this is a class method).
 |  
 |  llooaadd__wwoorrdd22vveecc__ffoorrmmaatt(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type
 |      Load the input-hidden weight matrix from the original C word2vec-tool format.
 |      
 |      Warnings
 |      --------
 |      The information stored in the file is incomplete (the binary tree is missing),
 |      so while you can query for word similarity etc., you cannot continue training
 |      with a model loaded this way.
 |      
 |      Parameters
 |      ----------
 |      fname : str
 |          The file path to the saved word2vec-format file.
 |      fvocab : str, optional
 |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set
 |          (this is the file generated by `-save-vocab` flag of the original C tool).
 |      binary : bool, optional
 |          If True, indicates whether the data is in binary word2vec format.
 |      encoding : str, optional
 |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.
 |      unicode_errors : str, optional
 |          default 'strict', is a string suitable to be passed as the `errors`
 |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source
 |          file may include word tokens truncated in the middle of a multibyte unicode character
 |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.
 |      limit : int, optional
 |          Sets a maximum number of word-vectors to read from the file. The default,
 |          None, means read all.
 |      datatype : type, optional
 |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.
 |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)
 |      
 |      Returns
 |      -------
 |      :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`
 |          Loaded model.
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from WordEmbeddingsKeyedVectors:
 |  
 |  ____ccoonnttaaiinnss____(self, word)
 |  
 |  ____iinniitt____(self, vector_size)
 |      Initialize self.  See help(type(self)) for accurate signature.
 |  
 |  aaccccuurraaccyy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x7f4ef1a1b6a8>, case_insensitive=True)
 |      Compute accuracy of the model.
 |      
 |      The accuracy is reported (=printed to log and returned as a list) for each
 |      section separately, plus there's one aggregate summary at the end.
 |      
 |      Parameters
 |      ----------
 |      questions : str
 |          Path to file, where lines are 4-tuples of words, split into sections by ": SECTION NAME" lines.
 |          See `gensim/test/test_data/questions-words.txt` as example.
 |      restrict_vocab : int, optional
 |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.
 |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard
 |          in modern word embedding models).
 |      most_similar : function, optional
 |          Function used for similarity calculation.
 |      case_insensitive : bool, optional
 |          If True - convert all words to their uppercase form before evaluating the performance.
 |          Useful to handle case-mismatch between training tokens and words in the test set.
 |          In case of multiple case variants of a single word, the vector for the first occurrence
 |          (also the most frequent if vocabulary is sorted) is taken.
 |      
 |      Returns
 |      -------
 |      list of dict of (str, (str, str, str)
 |          Full lists of correct and incorrect predictions divided by sections.
 |  
 |  ddiissttaannccee(self, w1, w2)
 |      Compute cosine distance between two words.
 |      Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.
 |      
 |      Parameters
 |      ----------
 |      w1 : str
 |          Input word.
 |      w2 : str
 |          Input word.
 |      
 |      Returns
 |      -------
 |      float
 |          Distance between `w1` and `w2`.
 |  
 |  ddiissttaanncceess(self, word_or_vector, other_words=())
 |      Compute cosine distances from given word or vector to all words in `other_words`.
 |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.
 |      
 |      Parameters
 |      ----------
 |      word_or_vector : {str, numpy.ndarray}
 |          Word or vector from which distances are to be computed.
 |      other_words : iterable of str
 |          For each word in `other_words` distance from `word_or_vector` is computed.
 |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).
 |      
 |      Returns
 |      -------
 |      numpy.array
 |          Array containing distances to all words in `other_words` from input `word_or_vector`.
 |      
 |      Raises
 |      -----
 |      KeyError
 |          If either `word_or_vector` or any word in `other_words` is absent from vocab.
 |  
 |  ddooeessnntt__mmaattcchh(self, words)
 |      Which word from the given list doesn't go with the others?
 |      
 |      Parameters
 |      ----------
 |      words : list of str
 |          List of words.
 |      
 |      Returns
 |      -------
 |      str
 |          The word further away from the mean of all words.
 |  
 |  eevvaalluuaattee__wwoorrdd__aannaallooggiieess(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)
 |      Compute performance of the model on an analogy test set.
 |      
 |      This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see
 |      `discussion on GitHub #1935 <https://github.com/RaRe-Technologies/gensim/pull/1935>`_.
 |      
 |      The accuracy is reported (printed to log and returned as a score) for each section separately,
 |      plus there's one aggregate summary at the end.
 |      
 |      This method corresponds to the `compute-accuracy` script of the original C word2vec.
 |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.
 |      
 |      Parameters
 |      ----------
 |      analogies : str
 |          Path to file, where lines are 4-tuples of words, split into sections by ": SECTION NAME" lines.
 |          See `gensim/test/test_data/questions-words.txt` as example.
 |      restrict_vocab : int, optional
 |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.
 |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard
 |          in modern word embedding models).
 |      case_insensitive : bool, optional
 |          If True - convert all words to their uppercase form before evaluating the performance.
 |          Useful to handle case-mismatch between training tokens and words in the test set.
 |          In case of multiple case variants of a single word, the vector for the first occurrence
 |          (also the most frequent if vocabulary is sorted) is taken.
 |      dummy4unknown : bool, optional
 |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.
 |          Otherwise, these tuples are skipped entirely and not used in the evaluation.
 |      
 |      Returns
 |      -------
 |      score : float
 |          The overall evaluation score on the entire evaluation set
 |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}
 |          Results broken down by each section of the evaluation set. Each dict contains the name of the section
 |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the
 |          keys 'correct' and 'incorrect'.
 |  
 |  eevvaalluuaattee__wwoorrdd__ppaaiirrss(self, pairs, delimiter='\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)
 |      Compute correlation of the model with human similarity judgments.
 |      
 |      Notes
 |      -----
 |      More datasets can be found at
 |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html
 |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.
 |      
 |      Parameters
 |      ----------
 |      pairs : str
 |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.
 |          See `test/test_data/wordsim353.tsv` as example.
 |      delimiter : str, optional
 |          Separator in `pairs` file.
 |      restrict_vocab : int, optional
 |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.
 |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard
 |          in modern word embedding models).
 |      case_insensitive : bool, optional
 |          If True - convert all words to their uppercase form before evaluating the performance.
 |          Useful to handle case-mismatch between training tokens and words in the test set.
 |          In case of multiple case variants of a single word, the vector for the first occurrence
 |          (also the most frequent if vocabulary is sorted) is taken.
 |      dummy4unknown : bool, optional
 |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.
 |          Otherwise, these tuples are skipped entirely and not used in the evaluation.
 |      
 |      Returns
 |      -------
 |      pearson : tuple of (float, float)
 |          Pearson correlation coefficient with 2-tailed p-value.
 |      spearman : tuple of (float, float)
 |          Spearman rank-order correlation coefficient between the similarities from the dataset and the
 |          similarities produced by the model itself, with 2-tailed p-value.
 |      oov_ratio : float
 |          The ratio of pairs with unknown words.
 |  
 |  ggeett__vveeccttoorr(self, word)
 |      Get the entity's representations in vector space, as a 1D numpy array.
 |      
 |      Parameters
 |      ----------
 |      entity : str
 |          Identifier of the entity to return the vector for.
 |      
 |      Returns
 |      -------
 |      numpy.ndarray
 |          Vector for the specified entity.
 |      
 |      Raises
 |      ------
 |      KeyError
 |          If the given entity identifier doesn't exist.
 |  
 |  iinniitt__ssiimmss(self, replace=False)
 |      Precompute L2-normalized vectors.
 |      
 |      Parameters
 |      ----------
 |      replace : bool, optional
 |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!
 |      
 |      Warnings
 |      --------
 |      You **cannot continue training** after doing a replace.
 |      The model becomes effectively read-only: you can call
 |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,
 |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.
 |  
 |  mmoosstt__ssiimmiillaarr(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)
 |      Find the top-N most similar words.
 |      Positive words contribute positively towards the similarity, negative words negatively.
 |      
 |      This method computes cosine similarity between a simple mean of the projection
 |      weight vectors of the given words and the vectors for each word in the model.
 |      The method corresponds to the `word-analogy` and `distance` scripts in the original
 |      word2vec implementation.
 |      
 |      Parameters
 |      ----------
 |      positive : list of str, optional
 |          List of words that contribute positively.
 |      negative : list of str, optional
 |          List of words that contribute negatively.
 |      topn : int, optional
 |          Number of top-N similar words to return.
 |      restrict_vocab : int, optional
 |          Optional integer which limits the range of vectors which
 |          are searched for most-similar values. For example, restrict_vocab=10000 would
 |          only check the first 10000 word vectors in the vocabulary order. (This may be
 |          meaningful if you've sorted the vocabulary by descending frequency.)
 |      
 |      Returns
 |      -------
 |      list of (str, float)
 |          Sequence of (word, similarity).
 |  
 |  mmoosstt__ssiimmiillaarr__ccoossmmuull(self, positive=None, negative=None, topn=10)
 |      Find the top-N most similar words, using the multiplicative combination objective,
 |      proposed by `Omer Levy and Yoav Goldberg "Linguistic Regularities in Sparse and Explicit Word Representations"
 |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,
 |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.
 |      In the common analogy-solving case, of two positive and one negative examples,
 |      this method is equivalent to the "3CosMul" objective (equation (4)) of Levy and Goldberg.
 |      
 |      Additional positive or negative examples contribute to the numerator or denominator,
 |      respectively - a potentially sensible but untested extension of the method.
 |      With a single positive example, rankings will be the same as in the default
 |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.
 |      
 |      Parameters
 |      ----------
 |      positive : list of str, optional
 |          List of words that contribute positively.
 |      negative : list of str, optional
 |          List of words that contribute negatively.
 |      topn : int, optional
 |          Number of top-N similar words to return.
 |      
 |      Returns
 |      -------
 |      list of (str, float)
 |          Sequence of (word, similarity).
 |  
 |  nn__ssiimmiillaarriittyy(self, ws1, ws2)
 |      Compute cosine similarity between two sets of words.
 |      
 |      Parameters
 |      ----------
 |      ws1 : list of str
 |          Sequence of words.
 |      ws2: list of str
 |          Sequence of words.
 |      
 |      Returns
 |      -------
 |      numpy.ndarray
 |          Similarities between `ws1` and `ws2`.
 |  
 |  rreellaattiivvee__ccoossiinnee__ssiimmiillaarriittyy(self, wa, wb, topn=10)
 |      Compute the relative cosine similarity between two words given top-n similar words,
 |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc "A Minimally Supervised Approach
 |      for Synonym Extraction with Word Embeddings" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.
 |      
 |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.
 |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than
 |      any arbitrary word pairs.
 |      
 |      Parameters
 |      ----------
 |      wa: str
 |          Word for which we have to look top-n similar word.
 |      wb: str
 |          Word for which we evaluating relative cosine similarity with wa.
 |      topn: int, optional
 |          Number of top-n similar words to look with respect to wa.
 |      
 |      Returns
 |      -------
 |      numpy.float64
 |          Relative cosine similarity between wa and wb.
 |  
 |  ssaavvee(self, *args, **kwargs)
 |      Save KeyedVectors.
 |      
 |      Parameters
 |      ----------
 |      fname : str
 |          Path to the output file.
 |      
 |      See Also
 |      --------
 |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`
 |          Load saved model.
 |  
 |  ssiimmiillaarr__bbyy__vveeccttoorr(self, vector, topn=10, restrict_vocab=None)
 |      Find the top-N most similar words by vector.
 |      
 |      Parameters
 |      ----------
 |      vector : numpy.array
 |          Vector from which similarities are to be computed.
 |      topn : {int, False}, optional
 |          Number of top-N similar words to return. If topn is False, similar_by_vector returns
 |          the vector of similarity scores.
 |      restrict_vocab : int, optional
 |          Optional integer which limits the range of vectors which
 |          are searched for most-similar values. For example, restrict_vocab=10000 would
 |          only check the first 10000 word vectors in the vocabulary order. (This may be
 |          meaningful if you've sorted the vocabulary by descending frequency.)
 |      
 |      Returns
 |      -------
 |      list of (str, float)
 |          Sequence of (word, similarity).
 |  
 |  ssiimmiillaarr__bbyy__wwoorrdd(self, word, topn=10, restrict_vocab=None)
 |      Find the top-N most similar words.
 |      
 |      Parameters
 |      ----------
 |      word : str
 |          Word
 |      topn : {int, False}, optional
 |          Number of top-N similar words to return. If topn is False, similar_by_word returns
 |          the vector of similarity scores.
 |      restrict_vocab : int, optional
 |          Optional integer which limits the range of vectors which
 |          are searched for most-similar values. For example, restrict_vocab=10000 would
 |          only check the first 10000 word vectors in the vocabulary order. (This may be
 |          meaningful if you've sorted the vocabulary by descending frequency.)
 |      
 |      Returns
 |      -------
 |      list of (str, float)
 |          Sequence of (word, similarity).
 |  
 |  ssiimmiillaarriittyy(self, w1, w2)
 |      Compute cosine similarity between two words.
 |      
 |      Parameters
 |      ----------
 |      w1 : str
 |          Input word.
 |      w2 : str
 |          Input word.
 |      
 |      Returns
 |      -------
 |      float
 |          Cosine similarity between `w1` and `w2`.
 |  
 |  ssiimmiillaarriittyy__mmaattrriixx(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)
 |      Construct a term similarity matrix for computing Soft Cosine Measure.
 |      
 |      This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing
 |      Soft Cosine Measure between documents.
 |      
 |      Parameters
 |      ----------
 |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`
 |          A dictionary that specifies the considered terms.
 |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional
 |          A model that specifies the relative importance of the terms in the dictionary. The
 |          columns of the term similarity matrix will be build in a decreasing order of importance
 |          of terms, or in the order of term identifiers if None.
 |      threshold : float, optional
 |          Only embeddings more similar than `threshold` are considered when retrieving word
 |          embeddings closest to a given word embedding.
 |      exponent : float, optional
 |          Take the word embedding similarities larger than `threshold` to the power of `exponent`.
 |      nonzero_limit : int, optional
 |          The maximum number of non-zero elements outside the diagonal in a single column of the
 |          sparse term similarity matrix.
 |      dtype : numpy.dtype, optional
 |          Data-type of the sparse term similarity matrix.
 |      
 |      Returns
 |      -------
 |      :class:`scipy.sparse.csc_matrix`
 |          Term similarity matrix.
 |      
 |      See Also
 |      --------
 |      :func:`gensim.matutils.softcossim`
 |          The Soft Cosine Measure.
 |      :class:`~gensim.similarities.docsim.SoftCosineSimilarity`
 |          A class for performing corpus-based similarity queries with Soft Cosine Measure.
 |      
 |      Notes
 |      -----
 |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of
 |      `Delphine Charlet and Geraldine Damnati, "SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity
 |      between Questions for Community Question Answering", 2017
 |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`_.
 |  
 |  wwmmddiissttaannccee(self, document1, document2)
 |      Compute the Word Mover's Distance between two documents.
 |      
 |      When using this code, please consider citing the following papers:
 |      
 |      * `Ofir Pele and Michael Werman "A linear time histogram metric for improved SIFT matching"
 |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_
 |      * `Ofir Pele and Michael Werman "Fast and robust earth mover's distances"
 |        <https://ieeexplore.ieee.org/document/5459199/>`_
 |      * `Matt Kusner et al. "From Word Embeddings To Document Distances"
 |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.
 |      
 |      Parameters
 |      ----------
 |      document1 : list of str
 |          Input document.
 |      document2 : list of str
 |          Input document.
 |      
 |      Returns
 |      -------
 |      float
 |          Word Mover's distance between `document1` and `document2`.
 |      
 |      Warnings
 |      --------
 |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.
 |      
 |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)
 |      will be returned.
 |      
 |      Raises
 |      ------
 |      ImportError
 |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.
 |  
 |  wwoorrdd__vveecc(self, word, use_norm=False)
 |      Get `word` representations in vector space, as a 1D numpy array.
 |      
 |      Parameters
 |      ----------
 |      word : str
 |          Input word
 |      use_norm : bool, optional
 |          If True - resulting vector will be L2-normalized (unit euclidean length).
 |      
 |      Returns
 |      -------
 |      numpy.ndarray
 |          Vector representation of `word`.
 |      
 |      Raises
 |      ------
 |      KeyError
 |          If word not in vocabulary.
 |  
 |  wwoorrddss__cclloosseerr__tthhaann(self, w1, w2)
 |      Get all words that are closer to `w1` than `w2` is to `w1`.
 |      
 |      Parameters
 |      ----------
 |      w1 : str
 |          Input word.
 |      w2 : str
 |          Input word.
 |      
 |      Returns
 |      -------
 |      list (str)
 |          List of words that are closer to `w1` than `w2` is to `w1`.
 |  
 |  ----------------------------------------------------------------------
 |  Static methods inherited from WordEmbeddingsKeyedVectors:
 |  
 |  ccoossiinnee__ssiimmiillaarriittiieess(vector_1, vectors_all)
 |      Compute cosine similarities between one vector and a set of other vectors.
 |      
 |      Parameters
 |      ----------
 |      vector_1 : numpy.ndarray
 |          Vector from which similarities are to be computed, expected shape (dim,).
 |      vectors_all : numpy.ndarray
 |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).
 |      
 |      Returns
 |      -------
 |      numpy.ndarray
 |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).
 |  
 |  lloogg__aaccccuurraaccyy(section)
 |  
 |  lloogg__eevvaalluuaattee__wwoorrdd__ppaaiirrss(pearson, spearman, oov, pairs)
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from WordEmbeddingsKeyedVectors:
 |  
 |  iinnddeexx22eennttiittyy
 |  
 |  ssyynn00
 |  
 |  ssyynn00nnoorrmm
 |  
 |  wwvv
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from BaseKeyedVectors:
 |  
 |  ____ggeettiitteemm____(self, entities)
 |      Get vector representation of `entities`.
 |      
 |      Parameters
 |      ----------
 |      entities : {str, list of str}
 |          Input entity/entities.
 |      
 |      Returns
 |      -------
 |      numpy.ndarray
 |          Vector representation for `entities` (1D if `entities` is string, otherwise - 2D).
 |  
 |  ____sseettiitteemm____(self, entities, weights)
 |      Add entities and theirs vectors in a manual way.
 |      If some entity is already in the vocabulary, old vector is replaced with the new one.
 |      This method is alias for :meth:`~gensim.models.keyedvectors.BaseKeyedVectors.add` with `replace=True`.
 |      
 |      Parameters
 |      ----------
 |      entities : {str, list of str}
 |          Entities specified by their string ids.
 |      weights: {list of numpy.ndarray, numpy.ndarray}
 |          List of 1D np.array vectors or 2D np.array of vectors.
 |  
 |  aadddd(self, entities, weights, replace=False)
 |      Append entities and theirs vectors in a manual way.
 |      If some entity is already in the vocabulary, the old vector is kept unless `replace` flag is True.
 |      
 |      Parameters
 |      ----------
 |      entities : list of str
 |          Entities specified by string ids.
 |      weights: {list of numpy.ndarray, numpy.ndarray}
 |          List of 1D np.array vectors or a 2D np.array of vectors.
 |      replace: bool, optional
 |          Flag indicating whether to replace vectors for entities which already exist in the vocabulary,
 |          if True - replace vectors, otherwise - keep old vectors.
 |  
 |  cclloosseerr__tthhaann(self, entity1, entity2)
 |      Get all entities that are closer to `entity1` than `entity2` is to `entity1`.
 |  
 |  mmoosstt__ssiimmiillaarr__ttoo__ggiivveenn(self, entity1, entities_list)
 |      Get the `entity` from `entities_list` most similar to `entity1`.
 |  
 |  rraannkk(self, entity1, entity2)
 |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from gensim.utils.SaveLoad:
 |  
 |  ____ddiicctt____
 |      dictionary for instance variables (if defined)
 |  
 |  ____wweeaakkrreeff____
 |      list of weak references to the object (if defined)
